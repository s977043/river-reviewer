# promptfoo evaluation configuration
# See https://www.promptfoo.dev/docs/configuration/guide

description: 'Evaluation for [Skill Name]'

# Model providers to test
providers:
  - id: openai:gpt-4o-mini
    config:
      temperature: 0.1
  # - id: openai:gpt-4o
  #   config:
  #     temperature: 0.1

# Prompts to evaluate
prompts:
  - file://../prompt/system.md
  - file://../prompt/user.md

# Test cases
tests:
  # Test case 1: Valid finding
  - vars:
      diff: |
        --- a/src/example.ts
        +++ b/src/example.ts
        @@ -1,3 +1,5 @@
        +function foo() {
        +  // TODO: implement
        +}
    assert:
      - type: contains
        value: 'src/example.ts'
      - type: not-contains
        value: 'false positive'

  # Test case 2: No issues (clean code)
  - vars:
      diff: |
        --- a/src/clean.ts
        +++ b/src/clean.ts
        @@ -1,3 +1,5 @@
        +function bar() {
        +  return "clean implementation";
        +}
    assert:
      - type: contains-any
        value:
          - '問題ありません'
          - '指摘なし'
          - 'No findings'

# Output format
outputPath: ./output.json

# Default test settings
defaultTest:
  options:
    provider: openai:gpt-4o-mini
